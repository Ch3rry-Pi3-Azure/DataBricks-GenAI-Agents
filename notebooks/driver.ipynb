{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6618819-fc51-4803-bd32-90176cf4b7f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Tool-calling Agent\n",
    "\n",
    "This is an auto-generated notebook created by an AI playground export. In this notebook, you will:\n",
    "- Author a tool-calling [MLflow's `ResponsesAgent`](https://mlflow.org/docs/latest/api_reference/python_api/mlflow.pyfunc.html#mlflow.pyfunc.ResponsesAgent) that uses the OpenAI client\n",
    "- Manually test the agent's output\n",
    "- Evaluate the agent with Mosaic AI Agent Evaluation\n",
    "- Log and deploy the agent\n",
    "\n",
    "This notebook should be run on the `GenAI Agents Cluster` (preferred) or another cluster with DBR < 17. If serverless is enabled, select a cluster before running.\n",
    "\n",
    " **_NOTE:_**  This notebook uses the OpenAI SDK, but AI Agent Framework is compatible with any agent authoring framework, including LlamaIndex or LangGraph. To learn more, see the [Authoring Agents](https://learn.microsoft.com/azure/databricks/generative-ai/agent-framework/author-agent) Databricks documentation.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Address all `TODO`s in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2491bba-1f23-45b8-a214-ae030344f75d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define the agent in code\n",
    "Below we define our agent code in a single cell, enabling us to easily write it to a local Python file for subsequent logging and deployment using the `%%writefile` magic command.\n",
    "\n",
    "For more examples of tools to add to your agent, see [docs](https://learn.microsoft.com/azure/databricks/generative-ai/agent-framework/agent-tool)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0055822f-b869-42fd-a7bd-5ab4d526ec5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile agent.py\n",
    "import json\n",
    "from typing import Any, Callable, Generator, Optional\n",
    "from uuid import uuid4\n",
    "import warnings\n",
    "\n",
    "import mlflow\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks_openai import UCFunctionToolkit\n",
    "from mlflow.entities import SpanType\n",
    "from mlflow.pyfunc import ResponsesAgent\n",
    "from mlflow.types.responses import (\n",
    "    ResponsesAgentRequest,\n",
    "    ResponsesAgentResponse,\n",
    "    ResponsesAgentStreamEvent,\n",
    "    output_to_responses_items_stream,\n",
    "    to_chat_completions_input,\n",
    ")\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "from unitycatalog.ai.core.base import get_uc_function_client\n",
    "\n",
    "############################################\n",
    "# Define your LLM endpoint and system prompt\n",
    "############################################\n",
    "LLM_ENDPOINT_NAME = \"databricks-gpt-oss-120b\"\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a helpful Databricks assistant. \"\n",
    "    \"Use tools when they add value, and respond concisely. \"\n",
    "    \"If a tool call fails or data is missing, say so.\"\n",
    ")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "## Define tools for your agent, enabling it to retrieve data or take actions\n",
    "## beyond text generation\n",
    "## To create and see usage examples of more tools, see\n",
    "## https://learn.microsoft.com/azure/databricks/generative-ai/agent-framework/agent-tool\n",
    "###############################################################################\n",
    "class ToolInfo(BaseModel):\n",
    "    \"\"\"\n",
    "    Class representing a tool for the agent.\n",
    "    - \"name\" (str): The name of the tool.\n",
    "    - \"spec\" (dict): JSON description of the tool (matches OpenAI Responses format)\n",
    "    - \"exec_fn\" (Callable): Function that implements the tool logic\n",
    "    \"\"\"\n",
    "\n",
    "    name: str\n",
    "    spec: dict\n",
    "    exec_fn: Callable\n",
    "\n",
    "\n",
    "def create_tool_info(tool_spec, exec_fn_param: Optional[Callable] = None):\n",
    "    tool_spec[\"function\"].pop(\"strict\", None)\n",
    "    tool_name = tool_spec[\"function\"][\"name\"]\n",
    "    udf_name = tool_name.replace(\"__\", \".\")\n",
    "\n",
    "    # Define a wrapper that accepts kwargs for the UC tool call,\n",
    "    # then passes them to the UC tool execution client\n",
    "    def exec_fn(**kwargs):\n",
    "        function_result = uc_function_client.execute_function(udf_name, kwargs)\n",
    "        if function_result.error is not None:\n",
    "            return function_result.error\n",
    "        else:\n",
    "            return function_result.value\n",
    "    return ToolInfo(name=tool_name, spec=tool_spec, exec_fn=exec_fn_param or exec_fn)\n",
    "\n",
    "\n",
    "TOOL_INFOS = []\n",
    "\n",
    "# You can use UDFs in Unity Catalog as agent tools\n",
    "# TODO: Add additional tools\n",
    "UC_TOOL_NAMES = [\"system.ai.python_exec\"]\n",
    "\n",
    "uc_toolkit = UCFunctionToolkit(function_names=UC_TOOL_NAMES)\n",
    "uc_function_client = get_uc_function_client()\n",
    "for tool_spec in uc_toolkit.tools:\n",
    "    TOOL_INFOS.append(create_tool_info(tool_spec))\n",
    "\n",
    "\n",
    "# Custom tools defined in this notebook\n",
    "def get_utc_timestamp(**_):\n",
    "    from datetime import datetime, timezone\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "\n",
    "def word_count(text: str, **_):\n",
    "    return len(text.split())\n",
    "\n",
    "\n",
    "def echo_text(text: str, **_):\n",
    "    return text\n",
    "\n",
    "\n",
    "CUSTOM_TOOLS = [\n",
    "    (\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_utc_timestamp\",\n",
    "                \"description\": \"Return the current UTC timestamp in ISO 8601 format.\",\n",
    "                \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []},\n",
    "            },\n",
    "        },\n",
    "        get_utc_timestamp,\n",
    "    ),\n",
    "    (\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"word_count\",\n",
    "                \"description\": \"Count the number of words in the provided text.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\"text\": {\"type\": \"string\"}},\n",
    "                    \"required\": [\"text\"],\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "        word_count,\n",
    "    ),\n",
    "    (\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"echo_text\",\n",
    "                \"description\": \"Echo the provided text.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\"text\": {\"type\": \"string\"}},\n",
    "                    \"required\": [\"text\"],\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "        echo_text,\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "for tool_spec, exec_fn in CUSTOM_TOOLS:\n",
    "    TOOL_INFOS.append(create_tool_info(tool_spec, exec_fn_param=exec_fn))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ToolCallingAgent(ResponsesAgent):\n",
    "    \"\"\"\n",
    "    Class representing a tool-calling Agent\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, llm_endpoint: str, tools: list[ToolInfo]):\n",
    "        \"\"\"Initializes the ToolCallingAgent with tools.\"\"\"\n",
    "        self.llm_endpoint = llm_endpoint\n",
    "        self.workspace_client = WorkspaceClient()\n",
    "        self.model_serving_client: OpenAI = (\n",
    "            self.workspace_client.serving_endpoints.get_open_ai_client()\n",
    "        )\n",
    "        self._tools_dict = {tool.name: tool for tool in tools}\n",
    "\n",
    "    def get_tool_specs(self) -> list[dict]:\n",
    "        \"\"\"Returns tool specifications in the format OpenAI expects.\"\"\"\n",
    "        return [tool_info.spec for tool_info in self._tools_dict.values()]\n",
    "\n",
    "    @mlflow.trace(span_type=SpanType.TOOL)\n",
    "    def execute_tool(self, tool_name: str, args: dict) -> Any:\n",
    "        \"\"\"Executes the specified tool with the given arguments.\"\"\"\n",
    "        return self._tools_dict[tool_name].exec_fn(**args)\n",
    "\n",
    "    def call_llm(self, messages: list[dict[str, Any]]) -> Generator[dict[str, Any], None, None]:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", message=\"PydanticSerializationUnexpectedValue\")\n",
    "            for chunk in self.model_serving_client.chat.completions.create(\n",
    "                model=self.llm_endpoint,\n",
    "                messages=to_chat_completions_input(messages),\n",
    "                tools=self.get_tool_specs(),\n",
    "                stream=True,\n",
    "            ):\n",
    "                chunk_dict = chunk.to_dict()\n",
    "                if len(chunk_dict.get(\"choices\", [])) > 0:\n",
    "                    yield chunk_dict\n",
    "\n",
    "    def handle_tool_call(\n",
    "        self,\n",
    "        tool_call: dict[str, Any],\n",
    "        messages: list[dict[str, Any]],\n",
    "    ) -> ResponsesAgentStreamEvent:\n",
    "        \"\"\"\n",
    "        Execute tool calls, add them to the running message history, and return a ResponsesStreamEvent w/ tool output\n",
    "        \"\"\"\n",
    "        args = json.loads(tool_call[\"arguments\"])\n",
    "        result = str(self.execute_tool(tool_name=tool_call[\"name\"], args=args))\n",
    "\n",
    "        tool_call_output = self.create_function_call_output_item(tool_call[\"call_id\"], result)\n",
    "        messages.append(tool_call_output)\n",
    "        return ResponsesAgentStreamEvent(type=\"response.output_item.done\", item=tool_call_output)\n",
    "\n",
    "    def call_and_run_tools(\n",
    "        self,\n",
    "        messages: list[dict[str, Any]],\n",
    "        max_iter: int = 10,\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        for _ in range(max_iter):\n",
    "            last_msg = messages[-1]\n",
    "            if last_msg.get(\"role\", None) == \"assistant\":\n",
    "                return\n",
    "            elif last_msg.get(\"type\", None) == \"function_call\":\n",
    "                yield self.handle_tool_call(last_msg, messages)\n",
    "            else:\n",
    "                yield from output_to_responses_items_stream(\n",
    "                    chunks=self.call_llm(messages), aggregator=messages\n",
    "                )\n",
    "\n",
    "        yield ResponsesAgentStreamEvent(\n",
    "            type=\"response.output_item.done\",\n",
    "            item=self.create_text_output_item(\"Max iterations reached. Stopping.\", str(uuid4())),\n",
    "        )\n",
    "\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        outputs = [\n",
    "            event.item\n",
    "            for event in self.predict_stream(request)\n",
    "            if event.type == \"response.output_item.done\"\n",
    "        ]\n",
    "        return ResponsesAgentResponse(output=outputs, custom_outputs=request.custom_inputs)\n",
    "\n",
    "    def predict_stream(\n",
    "        self, request: ResponsesAgentRequest\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        messages = to_chat_completions_input([i.model_dump() for i in request.input])\n",
    "        if SYSTEM_PROMPT:\n",
    "            messages.insert(0, {\"role\": \"system\", \"content\": SYSTEM_PROMPT})\n",
    "        yield from self.call_and_run_tools(messages=messages)\n",
    "\n",
    "\n",
    "# Log the model using MLflow\n",
    "mlflow.openai.autolog()\n",
    "AGENT = ToolCallingAgent(llm_endpoint=LLM_ENDPOINT_NAME, tools=TOOL_INFOS)\n",
    "mlflow.models.set_model(AGENT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b8dc45a-b95c-478e-b141-1978ca688a11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Test the agent\n",
    "\n",
    "Interact with the agent to test its output. Since we manually traced methods within `ResponsesAgent`, you can view the trace for each step the agent takes, with any LLM calls made via the OpenAI SDK automatically traced by autologging.\n",
    "\n",
    "Replace this placeholder input with tool-focused examples that force tool usage so you can inspect traces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "423f0524-326a-4f4c-9000-1ee9489a4d37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `str` - serialized value may not be as expected [field_name='content', input_value=[{'type': 'reasoning', 's...15:19:35.573538+00:00'}], input_type=list])\n  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool_call: get_utc_timestamp args={}\ntool_output: 2026-01-02T15:19:35.573538+00:00\nassistant: 2026-01-02T15:19:35.573538+00:00\n---\ntool_call: word_count args={\n  \"text\": \"Databricks agents are fun to test\"\n}\ntool_output: 6\nassistant: The text contains **6 words**.\n---\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `str` - serialized value may not be as expected [field_name='content', input_value=[{'type': 'reasoning', 's...*tools are working.**'}], input_type=list])\n  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool_call: echo_text args={\n  \"text\": \"tools are working.\"\n}\ntool_output: tools are working.\nassistant: Here you go:\n\n**tools are working.**\n---\ntool_call: system__ai__python_exec args={\n  \"code\": \"print(19*23)\"\n}\ntool_output: 437\n\nassistant: 437\n---\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "[{\"trace_id\": \"tr-e3aa5ad6be99fc0d360d192882f8ab62\", \"sql_warehouse_id\": null}, {\"trace_id\": \"tr-1886efabd58ea41b53fea7ae97f86429\", \"sql_warehouse_id\": null}, {\"trace_id\": \"tr-7cefd471ac113c55aaea8feb37ea3e2b\", \"sql_warehouse_id\": null}, {\"trace_id\": \"tr-94e7b71ef3de82a4a8c98fb5dc05ccdf\", \"sql_warehouse_id\": null}]",
      "text/plain": [
       "[Trace(trace_id=tr-e3aa5ad6be99fc0d360d192882f8ab62), Trace(trace_id=tr-1886efabd58ea41b53fea7ae97f86429), Trace(trace_id=tr-7cefd471ac113c55aaea8feb37ea3e2b), Trace(trace_id=tr-94e7b71ef3de82a4a8c98fb5dc05ccdf)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "from agent import AGENT\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"PydanticSerializationUnexpectedValue\")\n",
    "\n",
    "tests = [\n",
    "    {\"role\": \"user\", \"content\": \"Use get_utc_timestamp to return the current UTC timestamp in ISO-8601.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Use word_count to count the words in: 'Databricks agents are fun to test'.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Use echo_text to repeat exactly: tools are working.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Using python_exec, compute 19*23 and return just the number.\"},\n",
    "]\n",
    "\n",
    "def summarize_response(response):\n",
    "    for item in response.output:\n",
    "        if item.type == \"function_call\":\n",
    "            print(f\"tool_call: {item.name} args={item.arguments}\")\n",
    "        elif item.type == \"function_call_output\":\n",
    "            print(f\"tool_output: {item.output}\")\n",
    "        elif item.type == \"message\":\n",
    "            text = \"\"\n",
    "            for chunk in item.content or []:\n",
    "                if isinstance(chunk, dict) and chunk.get(\"type\") == \"output_text\":\n",
    "                    text = chunk.get(\"text\", \"\")\n",
    "                    break\n",
    "            print(f\"assistant: {text}\")\n",
    "\n",
    "for t in tests:\n",
    "    resp = AGENT.predict({\"input\": [t]})\n",
    "    summarize_response(resp)\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "036308b6-d453-47a1-b6de-3cf96fec1109",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'response.output_item.done', 'item': {'type': 'function_call', 'id': 'chatcmpl_f696b2e9-2ccd-41da-94ec-073585b952d3', 'call_id': 'call_81bfb75e-1c92-495b-9a63-a6286e867e8e', 'name': 'system__ai__python_exec', 'arguments': '{\\n  \"code\": \"print(19*23)\"\\n}'}}\n{'type': 'response.output_item.done', 'item': {'type': 'function_call_output', 'call_id': 'call_81bfb75e-1c92-495b-9a63-a6286e867e8e', 'output': '437\\n'}}\n{'type': 'response.output_text.delta', 'item_id': 'chatcmpl_9cd25eb4-37e2-4306-aa6b-f1ab55c96eca', 'delta': ''}\n{'type': 'response.output_text.delta', 'item_id': 'chatcmpl_9cd25eb4-37e2-4306-aa6b-f1ab55c96eca', 'delta': '437'}\n{'type': 'response.output_text.delta', 'item_id': 'chatcmpl_9cd25eb4-37e2-4306-aa6b-f1ab55c96eca', 'delta': ''}\n{'type': 'response.output_item.done', 'item': {'id': 'chatcmpl_9cd25eb4-37e2-4306-aa6b-f1ab55c96eca', 'content': [{'text': '437', 'type': 'output_text'}], 'role': 'assistant', 'type': 'message'}}\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "{\"trace_id\": \"tr-17577afcc7d58f0d7d02b70cd876df3d\", \"sql_warehouse_id\": null}",
      "text/plain": [
       "Trace(trace_id=tr-17577afcc7d58f0d7d02b70cd876df3d)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for chunk in AGENT.predict_stream(\n",
    "    {\"input\": [{\"role\": \"user\", \"content\": \"Using python_exec, compute 19*23 and return just the number.\"}]}\n",
    "):\n",
    "    print(chunk.model_dump(exclude_none=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96db345a-6962-4777-83bb-e197db6504c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Log the `agent` as an MLflow model\n",
    "Determine Databricks resources to specify for automatic auth passthrough at deployment time\n",
    "- **TODO**: If your Unity Catalog Function queries a [vector search index](https://learn.microsoft.com/azure/databricks/generative-ai/agent-framework/unstructured-retrieval-tools) or leverages [external functions](https://learn.microsoft.com/azure/databricks/generative-ai/agent-framework/external-connection-tools), you need to include the dependent vector search index and UC connection objects, respectively, as resources. See [docs](https://learn.microsoft.com/azure/databricks/generative-ai/agent-framework/log-agent#specify-resources-for-automatic-authentication-passthrough) for more details.\n",
    "\n",
    "Log the agent as code from the `agent.py` file. See [MLflow - Models from Code](https://mlflow.org/docs/latest/models.html#models-from-code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b92091f0-de8c-4861-b2b1-b2918eb37558",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD17 View Logged Model at: https://adb-7405615340160596.16.azuredatabricks.net/ml/experiments/463397721036728/models/m-7cb2a565730e4d0cbcda83908c4ac991?o=7405615340160596\n2026/01/02 15:20:05 INFO mlflow.pyfunc: Predicting on input example to validate output\n/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `str` - serialized value may not be as expected [field_name='content', input_value=[{'type': 'reasoning', 's...mple text generation.'}], input_type=list])\n  return self.__pydantic_serializer__.to_python(\n/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `str` - serialized value may not be as expected [field_name='content', input_value=[{'type': 'reasoning', 's...nerating static text.'}], input_type=list])\n  return self.__pydantic_serializer__.to_python(\n"
     ]
    }
   ],
   "source": [
    "# Determine Databricks resources to specify for automatic auth passthrough at deployment time\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "import mlflow\n",
    "from agent import UC_TOOL_NAMES, LLM_ENDPOINT_NAME\n",
    "from mlflow.models.resources import DatabricksFunction, DatabricksServingEndpoint\n",
    "from pkg_resources import get_distribution\n",
    "\n",
    "logging.getLogger(\"mlflow.tracing.fluent\").setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\", message=\"PydanticSerializationUnexpectedValue\")\n",
    "\n",
    "resources = [DatabricksServingEndpoint(endpoint_name=LLM_ENDPOINT_NAME)]\n",
    "for tool_name in UC_TOOL_NAMES:\n",
    "    # TODO: If the UC function includes dependencies like external connection or vector search, please include them manually.\n",
    "    # See the TODO in the markdown above for more information.\n",
    "    resources.append(DatabricksFunction(function_name=tool_name))\n",
    "\n",
    "input_example = {\n",
    "    \"input\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is an LLM agent?\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "with mlflow.start_run():\n",
    "    logged_agent_info = mlflow.pyfunc.log_model(\n",
    "        name=\"agent\",\n",
    "        python_model=\"agent.py\",\n",
    "        input_example=input_example,\n",
    "        pip_requirements=[\n",
    "            \"databricks-openai\",\n",
    "            \"backoff\",\n",
    "            f\"databricks-connect=={get_distribution('databricks-connect').version}\",\n",
    "        ],\n",
    "        resources=resources,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4f522ec-cfb5-4046-a71c-6002325995cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Evaluate the agent with [Agent Evaluation](https://learn.microsoft.com/azure/databricks/mlflow3/genai/eval-monitor)\n",
    "\n",
    "You can edit the requests or expected responses in your evaluation dataset and run evaluation as you iterate your agent, leveraging mlflow to track the computed quality metrics.\n",
    "\n",
    "Evaluate your agent with one of our [predefined LLM scorers](https://learn.microsoft.com/azure/databricks/mlflow3/genai/eval-monitor/predefined-judge-scorers), or try adding [custom metrics](https://learn.microsoft.com/azure/databricks/mlflow3/genai/eval-monitor/custom-scorers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ec8b690-d244-4a81-a6fb-492fe717f360",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/02 15:20:15 INFO mlflow.genai.utils.data_validation: Testing model prediction with the first sample in the dataset. To disable this check, set the MLFLOW_GENAI_EVAL_SKIP_TRACE_VALIDATION environment variable to True.\n/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `str` - serialized value may not be as expected [field_name='content', input_value=[{'type': 'reasoning', 's...orms like Databricks.'}], input_type=list])\n  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38cd5c6006974ba791d0305a0688a092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [Elapsed: 00:00, Remaining: ?] "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `str` - serialized value may not be as expected [field_name='content', input_value=[{'type': 'reasoning', 's...generate static text.'}], input_type=list])\n  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <title>Evaluation output</title>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "    <style>\n",
       "        body {\n",
       "            font-family: Arial, sans-serif;\n",
       "        }\n",
       "\n",
       "        .header {\n",
       "            a.button {\n",
       "                padding: 4px 8px;\n",
       "                line-height: 20px;\n",
       "                box-shadow: none;\n",
       "                height: 20px;\n",
       "                display: inline-flex;\n",
       "                align-items: center;\n",
       "                justify-content: center;\n",
       "                vertical-align: middle;\n",
       "                background-color: rgb(34, 114, 180);\n",
       "                color: rgb(255, 255, 255);\n",
       "                text-decoration: none;\n",
       "                animation-duration: 0s;\n",
       "                transition: none 0s ease 0s;\n",
       "                position: relative;\n",
       "                white-space: nowrap;\n",
       "                text-align: center;\n",
       "                border: 1px solid rgb(192, 205, 216);\n",
       "                cursor: pointer;\n",
       "                user-select: none;\n",
       "                touch-action: manipulation;\n",
       "                border-radius: 4px;\n",
       "                gap: 6px;\n",
       "            }\n",
       "\n",
       "            a.button:hover {\n",
       "                background-color: rgb(14, 83, 139) !important;\n",
       "                border-color: transparent !important;\n",
       "                color: rgb(255, 255, 255) !important;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        .warnings-section {\n",
       "            margin-top: 8px;\n",
       "\n",
       "            ul {\n",
       "                list-style-type: none;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        .instructions-section {\n",
       "            margin-top: 16px;\n",
       "            font-size: 14px;\n",
       "\n",
       "            ul {\n",
       "                margin-top: 0;\n",
       "                margin-bottom: 0;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        code {\n",
       "            font-family: monospace;\n",
       "        }\n",
       "\n",
       "        .note {\n",
       "            color: #666;\n",
       "        }\n",
       "\n",
       "        a {\n",
       "            color: #2272B4;\n",
       "            text-decoration: none;\n",
       "        }\n",
       "\n",
       "        a:hover {\n",
       "            color: #005580;\n",
       "        }\n",
       "    </style>\n",
       "</head>\n",
       "<body>\n",
       "<div>\n",
       "    <div class=\"header\">\n",
       "        <a href=\"https://adb-7405615340160596.16.azuredatabricks.net/ml/experiments/463397721036728/evaluation-runs?selectedRunUuid=c3c7008970f04e2e92b2067fee58bf12\" class=\"button\">\n",
       "            View evaluation results in MLflow\n",
       "            <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"1em\" height=\"1em\" fill=\"none\" viewBox=\"0 0 16 16\" aria-hidden=\"true\" focusable=\"false\" class=\"\">\n",
       "                <path fill=\"currentColor\" d=\"M10 1h5v5h-1.5V3.56L8.53 8.53 7.47 7.47l4.97-4.97H10z\"></path>\n",
       "                <path fill=\"currentColor\" d=\"M1 2.75A.75.75 0 0 1 1.75 2H8v1.5H2.5v10h10V8H14v6.25a.75.75 0 0 1-.75.75H1.75a.75.75 0 0 1-.75-.75z\"></path>\n",
       "            </svg>\n",
       "        </a>\n",
       "    </div>\n",
       "</div>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow.genai.scorers import RelevanceToQuery, Safety, RetrievalRelevance, RetrievalGroundedness\n",
    "\n",
    "eval_dataset = [\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"input\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"What is an LLM agent?\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \"expected_response\": None\n",
    "    }\n",
    "]\n",
    "\n",
    "eval_results = mlflow.genai.evaluate(\n",
    "    data=eval_dataset,\n",
    "    predict_fn=lambda input: AGENT.predict({\"input\": input}),\n",
    "    scorers=[RelevanceToQuery(), Safety()], # add more scorers here if they're applicable\n",
    ")\n",
    "\n",
    "# Review the evaluation results in the MLfLow UI (see console output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "376e934c-5c32-4bf2-9a4e-a6c7d18f8025",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Perform pre-deployment validation of the agent\n",
    "Before registering and deploying the agent, we perform pre-deployment checks via the [mlflow.models.predict()](https://mlflow.org/docs/latest/python_api/mlflow.models.html#mlflow.models.predict) API. See [documentation](https://learn.microsoft.com/azure/databricks/machine-learning/model-serving/model-serving-debug#validate-inputs) for details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9572b4f-6a8b-44b1-b4fa-49cf28f1f621",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dbf013ce17640a29967165458ed0574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e9b6c86fad647f9bcf04487273997cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/02 15:20:32 INFO mlflow.models.flavor_backend_registry: Selected backend for flavor 'python_function'\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89cd70cb219e4e29beb1a24689227e11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14c925ba0e7346c5a5a11b77a12f3a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/02 15:20:34 INFO mlflow.utils.virtualenv: Environment /local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-19b7f-1bc68-5/mlflow/envs/virtualenv_envs/mlflow-e9d6f2e194b4f6fdbaf6eeed2aefd5e24332888f already exists\n2026/01/02 15:20:34 INFO mlflow.utils.environment: === Running command '['bash', '-c', 'source /local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-19b7f-1bc68-5/mlflow/envs/virtualenv_envs/mlflow-e9d6f2e194b4f6fdbaf6eeed2aefd5e24332888f/bin/activate && python -c \"\"']'\n2026/01/02 15:20:34 INFO mlflow.utils.environment: === Running command '['bash', '-c', 'source /local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-19b7f-1bc68-5/mlflow/envs/virtualenv_envs/mlflow-e9d6f2e194b4f6fdbaf6eeed2aefd5e24332888f/bin/activate && python /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/mlflow/pyfunc/_mlflow_pyfunc_backend_predict.py --model-uri file:///local_disk0/repl_tmp_data/ReplId-19b7f-1bc68-5/tmpukiy3ka2/agent --content-type json --input-path /local_disk0/repl_tmp_data/ReplId-19b7f-1bc68-5/tmpk7vi_deb/input.json']'\n/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-19b7f-1bc68-5/mlflow/envs/virtualenv_envs/mlflow-e9d6f2e194b4f6fdbaf6eeed2aefd5e24332888f/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `str` - serialized value may not be as expected [field_name='content', input_value=[{'type': 'reasoning', 's...can I help you today?'}], input_type=list])\n  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"object\": \"response\", \"output\": [{\"type\": \"reasoning\", \"summary\": [{\"type\": \"summary_text\", \"text\": \"The user just says \\\"Hello!\\\". As a Databricks assistant, we respond politely. No tool needed.\"}], \"id\": \"chatcmpl_3879e3ff-dae2-4d11-aef8-0d34e6228c13\"}, {\"type\": \"message\", \"id\": \"chatcmpl_3879e3ff-dae2-4d11-aef8-0d34e6228c13\", \"content\": [{\"text\": \"Hello! How can I help you today?\", \"type\": \"output_text\"}], \"role\": \"assistant\"}]}"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/02 15:20:49 INFO mlflow.tracing.export.async_export_queue: Flushing the async trace logging queue before program exit. This may take a while...\n"
     ]
    }
   ],
   "source": [
    "mlflow.models.predict(\n",
    "    model_uri=f\"runs:/{logged_agent_info.run_id}/agent\",\n",
    "    input_data={\"input\": [{\"role\": \"user\", \"content\": \"Hello!\"}]},\n",
    "    env_manager=\"uv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f789d166-5cb3-4f60-9625-c078e95cc913",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Register the model to Unity Catalog\n",
    "\n",
    "Update the `catalog`, `schema`, and `model_name` below to register the MLflow model to Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a864f9c9-218c-43a1-881d-82d8830b2a35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fdd362d4c4b4caba8e5fc35848e574f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "382fa801ab1e42c094ad2f9c396e1d8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model to adb_genai_evolved_teal.default.genai-agent\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'adb_genai_evolved_teal.default.genai-agent'.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed92f3410bc044ac9d841caecf2caef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e304247c97f40d28d03ebd1a8fceff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD17 Created version '1' of model 'adb_genai_evolved_teal.default.genai-agent': https://adb-7405615340160596.16.azuredatabricks.net/explore/data/models/adb_genai_evolved_teal/default/genai-agent/version/1?o=7405615340160596\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# Auto-select a usable catalog/schema for UC registration\n",
    "catalogs = [r.catalog for r in spark.sql(\"SHOW CATALOGS\").collect()]\n",
    "current_catalog = None\n",
    "try:\n",
    "    current_catalog = spark.sql(\"SELECT current_catalog()\").first()[0]\n",
    "except Exception:\n",
    "    current_catalog = None\n",
    "\n",
    "candidates = [c for c in catalogs if c not in (\"system\", \"samples\")]\n",
    "if current_catalog in candidates:\n",
    "    candidates.remove(current_catalog)\n",
    "    candidates.insert(0, current_catalog)\n",
    "\n",
    "schema = \"default\"\n",
    "catalog = None\n",
    "last_error = None\n",
    "\n",
    "def can_use_catalog(candidate):\n",
    "    try:\n",
    "        spark.sql(f\"USE CATALOG `{candidate}`\")\n",
    "        spark.sql(\"SHOW SCHEMAS\").collect()\n",
    "        return True\n",
    "    except Exception as exc:\n",
    "        return False\n",
    "\n",
    "for candidate in candidates:\n",
    "    if not can_use_catalog(candidate):\n",
    "        continue\n",
    "    try:\n",
    "        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{candidate}`.`{schema}`\")\n",
    "        catalog = candidate\n",
    "        break\n",
    "    except Exception as exc:\n",
    "        last_error = exc\n",
    "\n",
    "if catalog is None:\n",
    "    raise RuntimeError(\n",
    "        \"No usable catalog found for UC registration. \"\n",
    "        \"Ask an admin to grant USE CATALOG/CREATE SCHEMA, \"\n",
    "        f\"or check permissions. Last error: {last_error}\"\n",
    "    )\n",
    "\n",
    "model_name = \"genai-agent\"\n",
    "UC_MODEL_NAME = f\"{catalog}.{schema}.{model_name}\"\n",
    "print(f\"Registering model to {UC_MODEL_NAME}\")\n",
    "\n",
    "# register the model to UC\n",
    "uc_registered_model_info = mlflow.register_model(\n",
    "    model_uri=logged_agent_info.model_uri, name=UC_MODEL_NAME\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3292ad4a-9ba9-43ec-a357-c19a7552a678",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deploy the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a150401-ed92-4df6-9026-a34d46cc7f48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c84beb5ac29435ca3bec7033e23dd2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/databricks/agents/deployments.py:641: UserWarning: This endpoint is being deployed without a feedback model, which has been deprecated.\nFor more information, see: https://docs.databricks.com/aws/en/generative-ai/agent-framework/feedback-model\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n    Deployment of adb_genai_evolved_teal.default.genai-agent version 1 initiated.  This can take up to 15 minutes and the Review App & Query Endpoint will not work until this deployment finishes.\n\n    View status: https://adb-7405615340160596.16.azuredatabricks.net/ml/endpoints/agents_adb_genai_evolved_teal-default-genai-agent/?o=7405615340160596\n    Review App: https://adb-7405615340160596.16.azuredatabricks.net/ml/review-v2/439fae7ccbef4a6da8765701466c850d/chat?o=7405615340160596\n\nYou can refer back to the links above from the endpoint detail page at https://adb-7405615340160596.16.azuredatabricks.net/ml/endpoints/agents_adb_genai_evolved_teal-default-genai-agent/?o=7405615340160596.\n\nTo set up monitoring for your deployed agent, see:\nhttps://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/production-monitoring\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Deployment(model_name='adb_genai_evolved_teal.default.genai-agent', model_version='1', endpoint_name='agents_adb_genai_evolved_teal-default-genai-agent', served_entity_name='adb_genai_evolved_teal-default-genai-agent_1', query_endpoint='https://adb-7405615340160596.16.azuredatabricks.net/serving-endpoints/agents_adb_genai_evolved_teal-default-genai-agent/served-models/adb_genai_evolved_teal-default-genai-agent_1/invocations?o=7405615340160596', endpoint_url='https://adb-7405615340160596.16.azuredatabricks.net/ml/endpoints/agents_adb_genai_evolved_teal-default-genai-agent/?o=7405615340160596', review_app_url='https://adb-7405615340160596.16.azuredatabricks.net/ml/review-v2/439fae7ccbef4a6da8765701466c850d/chat?o=7405615340160596')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from databricks import agents\n",
    "# NOTE: pass scale_to_zero=True to agents.deploy() to enable scale-to-zero for cost savings.\n",
    "# This is not recommended for production workloads, as capacity is not guaranteed when scaled to zero.\n",
    "# Scaled to zero endpoints may take extra time to respond when queried, while they scale back up.\n",
    "agents.deploy(UC_MODEL_NAME, uc_registered_model_info.version, tags = {\"endpointSource\": \"playground\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b9cd6a7-234b-4f3d-a4b4-912670ae0134",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Next steps\n",
    "\n",
    "After your agent is deployed, you can chat with it in AI playground to perform additional checks, share it with SMEs in your organization for feedback, or embed it in a production application. See [docs](https://learn.microsoft.com/azure/databricks/generative-ai/deploy-agent) for details"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "driver.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}